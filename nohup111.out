2022-10-14 00:01:35 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='gelu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_sign', attention_dropout=0.0, best_checkpoint_metric='bleu', bpe=None, bucket_cap_mb=25, clip_norm=25, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/phoenix2014T/sp25000', dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=10, decoder_embed_dim=300, decoder_embed_path='data-bin/phoenix2014T/sp25000/emb', decoder_ffn_embed_dim=2048, decoder_input_dim=300, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=300, device_id=0, disable_bucketing=False, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.4, early_proj=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=1, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, eval_bleu=True, eval_bleu_args='{"beam": 10, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe='sentencepiece', eval_bleu_save=None, eval_tokenized_bleu=True, fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layer_wise_attention=False, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', log_format=None, log_interval=1000, lr=[0.0001], lr_mode='max', lr_patience=8, lr_schedule_metric='bleu', lr_scheduler='reduce_lr_on_plateau', lr_shrink=0.5, lr_threshold=0.0001, max_epoch=200, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1, multilv_args='{"span_lengths": [8, 12, 16], "level_links": [[1, 0], [2, 1], [2, 0]], "stride": 2, "eye": true, "same_level_links": true, "symmetric": true}', no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=True, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_token_positional_embeddings=False, num_levels=3, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='CHECKPOINT_PATH', save_interval=1, save_interval_updates=0, seed=1, sentence_avg=False, share_all_embeddings=False, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, src_lv0_body_feat_root='i3d-features/span=8_stride=2', src_lv0_lefthand_feat_root=None, src_lv0_righthand_feat_root=None, src_lv1_body_feat_root='i3d-features/span=12_stride=2', src_lv1_lefthand_feat_root=None, src_lv1_righthand_feat_root=None, src_lv2_body_feat_root='i3d-features/span=16_stride=2', src_lv2_lefthand_feat_root=None, src_lv2_righthand_feat_root=None, target_lang='de', task='translation_sign', tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, train_subset='train', update_freq=[1], use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='test', validate_interval=1, warmup_init_lr=-1, warmup_updates=0, weight_decay=0.0001)
2022-10-14 00:01:35 | INFO | root | [de] dictionary: 2656 types
2022-10-14 00:01:36 | INFO | fairseq.data.data_utils | loaded 642 examples from: data-bin/phoenix2014T/sp25000/test.sign-de.de
2022-10-14 00:01:36 | INFO | fairseq.tasks.translation_sign | data-bin/phoenix2014T/sp25000 test sign-de 642 examples
2022-10-14 00:01:36 | INFO | root | Loaded pretrained embeddings from data-bin/phoenix2014T/sp25000/emb
2022-10-14 00:01:37 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoderSign(
    (embed_positions): LearnedPositionalEmbedding(1025, 1024, padding_idx=0)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (embed_tokens): Embedding(2656, 300, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 300, padding_idx=1)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=300, out_features=300, bias=True)
          (v_proj): Linear(in_features=300, out_features=300, bias=True)
          (q_proj): Linear(in_features=300, out_features=300, bias=True)
          (out_proj): Linear(in_features=300, out_features=300, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=1024, out_features=300, bias=True)
          (v_proj): Linear(in_features=1024, out_features=300, bias=True)
          (q_proj): Linear(in_features=300, out_features=300, bias=True)
          (out_proj): Linear(in_features=300, out_features=300, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=300, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=300, bias=True)
        (final_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=300, out_features=300, bias=True)
          (v_proj): Linear(in_features=300, out_features=300, bias=True)
          (q_proj): Linear(in_features=300, out_features=300, bias=True)
          (out_proj): Linear(in_features=300, out_features=300, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=1024, out_features=300, bias=True)
          (v_proj): Linear(in_features=1024, out_features=300, bias=True)
          (q_proj): Linear(in_features=300, out_features=300, bias=True)
          (out_proj): Linear(in_features=300, out_features=300, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=300, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=300, bias=True)
        (final_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=300, out_features=300, bias=True)
          (v_proj): Linear(in_features=300, out_features=300, bias=True)
          (q_proj): Linear(in_features=300, out_features=300, bias=True)
          (out_proj): Linear(in_features=300, out_features=300, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=1024, out_features=300, bias=True)
          (v_proj): Linear(in_features=1024, out_features=300, bias=True)
          (q_proj): Linear(in_features=300, out_features=300, bias=True)
          (out_proj): Linear(in_features=300, out_features=300, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=300, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=300, bias=True)
        (final_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=300, out_features=300, bias=True)
          (v_proj): Linear(in_features=300, out_features=300, bias=True)
          (q_proj): Linear(in_features=300, out_features=300, bias=True)
          (out_proj): Linear(in_features=300, out_features=300, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=1024, out_features=300, bias=True)
          (v_proj): Linear(in_features=1024, out_features=300, bias=True)
          (q_proj): Linear(in_features=300, out_features=300, bias=True)
          (out_proj): Linear(in_features=300, out_features=300, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=300, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=300, bias=True)
        (final_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=300, out_features=300, bias=True)
          (v_proj): Linear(in_features=300, out_features=300, bias=True)
          (q_proj): Linear(in_features=300, out_features=300, bias=True)
          (out_proj): Linear(in_features=300, out_features=300, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=1024, out_features=300, bias=True)
          (v_proj): Linear(in_features=1024, out_features=300, bias=True)
          (q_proj): Linear(in_features=300, out_features=300, bias=True)
          (out_proj): Linear(in_features=300, out_features=300, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=300, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=300, bias=True)
        (final_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=300, out_features=300, bias=True)
          (v_proj): Linear(in_features=300, out_features=300, bias=True)
          (q_proj): Linear(in_features=300, out_features=300, bias=True)
          (out_proj): Linear(in_features=300, out_features=300, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=1024, out_features=300, bias=True)
          (v_proj): Linear(in_features=1024, out_features=300, bias=True)
          (q_proj): Linear(in_features=300, out_features=300, bias=True)
          (out_proj): Linear(in_features=300, out_features=300, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=300, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=300, bias=True)
        (final_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
)
2022-10-14 00:01:37 | INFO | fairseq_cli.train | model transformer_sign, criterion LabelSmoothedCrossEntropyCriterion
2022-10-14 00:01:37 | INFO | fairseq_cli.train | num. model params: 25689360 (num. trained: 25689360)
2022-10-14 00:01:42 | INFO | fairseq_cli.train | training on 1 GPUs
2022-10-14 00:01:42 | INFO | fairseq_cli.train | max tokens per GPU = 4096 and max sentences per GPU = None
2022-10-14 00:01:42 | INFO | fairseq.trainer | no existing checkpoint found CHECKPOINT_PATH/checkpoint_last.pt
2022-10-14 00:01:42 | INFO | fairseq.trainer | loading train data for epoch 0
2022-10-14 00:01:44 | INFO | fairseq.data.data_utils | loaded 7096 examples from: data-bin/phoenix2014T/sp25000/train.sign-de.de
2022-10-14 00:01:44 | INFO | fairseq.tasks.translation_sign | data-bin/phoenix2014T/sp25000 train sign-de 7096 examples
2022-10-14 00:01:44 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16
2022-10-14 00:11:58 | INFO | train | epoch 001 | loss 9.643 | nll_loss 9.33 | ppl 643.482 | wps 219 | ups 0.18 | wpb 1212.8 | bsz 63.9 | num_updates 111 | lr 0.0001 | gnorm 1.312 | clip 0 | oom 0 | train_wall 23 | wall 617
/home/tangli/TSPNet/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
Traceback (most recent call last):
  File "train.py", line 11, in <module>
    cli_main()
  File "/home/tangli/TSPNet/fairseq_cli/train.py", line 323, in cli_main
    main(args)
  File "/home/tangli/TSPNet/fairseq_cli/train.py", line 105, in main
    valid_losses = validate(args, trainer, task, epoch_itr, valid_subsets)
  File "/home/tangli/TSPNet/fairseq_cli/train.py", line 250, in validate
    trainer.valid_step(sample)
  File "/home/tangli/anaconda3/envs/TSPNet/lib/python3.6/contextlib.py", line 52, in inner
    return func(*args, **kwds)
  File "/home/tangli/TSPNet/fairseq/trainer.py", line 452, in valid_step
    raise e
  File "/home/tangli/TSPNet/fairseq/trainer.py", line 437, in valid_step
    sample, self.model, self.criterion
  File "/home/tangli/TSPNet/fairseq/tasks/translation_sign.py", line 239, in valid_step
    bleu, hyps, refs = self._inference_with_bleu(self.sequence_generator, sample, model)
  File "/home/tangli/TSPNet/fairseq/tasks/translation_sign.py", line 304, in _inference_with_bleu
    gen_out = self.inference_step(generator, [model], sample, None)
  File "/home/tangli/TSPNet/fairseq/tasks/fairseq_task.py", line 309, in inference_step
    return generator.generate(models, sample, prefix_tokens=prefix_tokens)
  File "/home/tangli/anaconda3/envs/TSPNet/lib/python3.6/site-packages/torch/autograd/grad_mode.py", line 26, in decorate_context
    return func(*args, **kwargs)
  File "/home/tangli/TSPNet/fairseq/sequence_generator.py", line 92, in generate
    return self._generate(model, sample, **kwargs)
  File "/home/tangli/anaconda3/envs/TSPNet/lib/python3.6/site-packages/torch/autograd/grad_mode.py", line 26, in decorate_context
    return func(*args, **kwargs)
  File "/home/tangli/TSPNet/fairseq/sequence_generator.py", line 361, in _generate
    scores.view(bsz, beam_size, -1)[:, :, :step],
  File "/home/tangli/TSPNet/fairseq/search.py", line 81, in step
    torch.div(self.indices_buf, vocab_size, out=self.beams_buf)
RuntimeError: result type Float can't be cast to the desired output type Long
