2022-10-14 13:12:40 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:13672
2022-10-14 13:12:40 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:13672
2022-10-14 13:12:40 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2022-10-14 13:12:40 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:13672
2022-10-14 13:12:40 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2022-10-14 13:12:40 | INFO | fairseq.distributed_utils | distributed init (rank 3): tcp://localhost:13672
2022-10-14 13:12:40 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2022-10-14 13:12:40 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2022-10-14 13:12:40 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-10-14 13:12:40 | INFO | fairseq.distributed_utils | initialized host root1-PowerEdge-T640 as rank 0
2022-10-14 13:12:40 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-10-14 13:12:40 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-10-14 13:12:40 | INFO | fairseq.distributed_utils | initialized host root1-PowerEdge-T640 as rank 1
2022-10-14 13:12:40 | INFO | fairseq.distributed_utils | initialized host root1-PowerEdge-T640 as rank 2
2022-10-14 13:12:40 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-10-14 13:12:40 | INFO | fairseq.distributed_utils | initialized host root1-PowerEdge-T640 as rank 3
2022-10-14 13:12:49 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='gelu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_sign', attention_dropout=0.0, best_checkpoint_metric='bleu', bpe=None, bucket_cap_mb=25, clip_norm=25, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/phoenix2014T/sp25000', dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=10, decoder_embed_dim=300, decoder_embed_path='data-bin/phoenix2014T/sp25000/emb', decoder_ffn_embed_dim=2048, decoder_input_dim=300, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=300, device_id=0, disable_bucketing=False, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:13672', distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=4, dropout=0.4, early_proj=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=1, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, eval_bleu=True, eval_bleu_args='{"beam": 10, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe='sentencepiece', eval_bleu_save=None, eval_tokenized_bleu=True, fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layer_wise_attention=False, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', log_format=None, log_interval=1000, lr=[0.0001], lr_mode='max', lr_patience=8, lr_schedule_metric='bleu', lr_scheduler='reduce_lr_on_plateau', lr_shrink=0.5, lr_threshold=0.0001, max_epoch=200, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1, multilv_args='{"span_lengths": [8, 12, 16], "level_links": [[1, 0], [2, 1], [2, 0]], "stride": 2, "eye": true, "same_level_links": true, "symmetric": true}', no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_token_positional_embeddings=False, num_levels=3, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='/home/tangli/TSPNet/CHECKPOINT_PATH/checkpoint_best.pt', save_dir='checkpoints', save_interval=200, save_interval_updates=0, seed=1, sentence_avg=False, share_all_embeddings=False, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, src_lv0_body_feat_root='i3d-features/span=8_stride=2', src_lv0_lefthand_feat_root=None, src_lv0_righthand_feat_root=None, src_lv1_body_feat_root='i3d-features/span=12_stride=2', src_lv1_lefthand_feat_root=None, src_lv1_righthand_feat_root=None, src_lv2_body_feat_root='i3d-features/span=16_stride=2', src_lv2_lefthand_feat_root=None, src_lv2_righthand_feat_root=None, target_lang='de', task='translation_sign', tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, train_subset='train', update_freq=[1], use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='test', validate_interval=1, warmup_init_lr=-1, warmup_updates=0, weight_decay=0.0001)
2022-10-14 13:12:49 | INFO | root | [de] dictionary: 2656 types
2022-10-14 13:12:49 | INFO | fairseq.data.data_utils | loaded 642 examples from: data-bin/phoenix2014T/sp25000/test.sign-de.de
2022-10-14 13:12:49 | INFO | fairseq.tasks.translation_sign | data-bin/phoenix2014T/sp25000 test sign-de 642 examples
2022-10-14 13:12:50 | INFO | root | Loaded pretrained embeddings from data-bin/phoenix2014T/sp25000/emb
2022-10-14 13:12:50 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoderSign(
    (embed_positions): LearnedPositionalEmbedding(1025, 1024, padding_idx=0)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (embed_tokens): Embedding(2656, 300, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 300, padding_idx=1)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=300, out_features=300, bias=True)
          (v_proj): Linear(in_features=300, out_features=300, bias=True)
          (q_proj): Linear(in_features=300, out_features=300, bias=True)
          (out_proj): Linear(in_features=300, out_features=300, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=1024, out_features=300, bias=True)
          (v_proj): Linear(in_features=1024, out_features=300, bias=True)
          (q_proj): Linear(in_features=300, out_features=300, bias=True)
          (out_proj): Linear(in_features=300, out_features=300, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=300, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=300, bias=True)
        (final_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=300, out_features=300, bias=True)
          (v_proj): Linear(in_features=300, out_features=300, bias=True)
          (q_proj): Linear(in_features=300, out_features=300, bias=True)
          (out_proj): Linear(in_features=300, out_features=300, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=1024, out_features=300, bias=True)
          (v_proj): Linear(in_features=1024, out_features=300, bias=True)
          (q_proj): Linear(in_features=300, out_features=300, bias=True)
          (out_proj): Linear(in_features=300, out_features=300, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=300, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=300, bias=True)
        (final_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=300, out_features=300, bias=True)
          (v_proj): Linear(in_features=300, out_features=300, bias=True)
          (q_proj): Linear(in_features=300, out_features=300, bias=True)
          (out_proj): Linear(in_features=300, out_features=300, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=1024, out_features=300, bias=True)
          (v_proj): Linear(in_features=1024, out_features=300, bias=True)
          (q_proj): Linear(in_features=300, out_features=300, bias=True)
          (out_proj): Linear(in_features=300, out_features=300, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=300, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=300, bias=True)
        (final_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=300, out_features=300, bias=True)
          (v_proj): Linear(in_features=300, out_features=300, bias=True)
          (q_proj): Linear(in_features=300, out_features=300, bias=True)
          (out_proj): Linear(in_features=300, out_features=300, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=1024, out_features=300, bias=True)
          (v_proj): Linear(in_features=1024, out_features=300, bias=True)
          (q_proj): Linear(in_features=300, out_features=300, bias=True)
          (out_proj): Linear(in_features=300, out_features=300, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=300, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=300, bias=True)
        (final_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=300, out_features=300, bias=True)
          (v_proj): Linear(in_features=300, out_features=300, bias=True)
          (q_proj): Linear(in_features=300, out_features=300, bias=True)
          (out_proj): Linear(in_features=300, out_features=300, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=1024, out_features=300, bias=True)
          (v_proj): Linear(in_features=1024, out_features=300, bias=True)
          (q_proj): Linear(in_features=300, out_features=300, bias=True)
          (out_proj): Linear(in_features=300, out_features=300, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=300, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=300, bias=True)
        (final_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=300, out_features=300, bias=True)
          (v_proj): Linear(in_features=300, out_features=300, bias=True)
          (q_proj): Linear(in_features=300, out_features=300, bias=True)
          (out_proj): Linear(in_features=300, out_features=300, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=1024, out_features=300, bias=True)
          (v_proj): Linear(in_features=1024, out_features=300, bias=True)
          (q_proj): Linear(in_features=300, out_features=300, bias=True)
          (out_proj): Linear(in_features=300, out_features=300, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=300, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=300, bias=True)
        (final_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
)
2022-10-14 13:12:50 | INFO | fairseq_cli.train | model transformer_sign, criterion LabelSmoothedCrossEntropyCriterion
2022-10-14 13:12:50 | INFO | fairseq_cli.train | num. model params: 25689360 (num. trained: 25689360)
2022-10-14 13:12:50 | INFO | fairseq_cli.train | training on 4 GPUs
2022-10-14 13:12:50 | INFO | fairseq_cli.train | max tokens per GPU = 4096 and max sentences per GPU = None
2022-10-14 13:12:51 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16
2022-10-14 13:12:52 | INFO | fairseq.trainer | loaded checkpoint /home/tangli/TSPNet/CHECKPOINT_PATH/checkpoint_best.pt (epoch 77 @ 8547 updates)
2022-10-14 13:12:52 | INFO | fairseq.trainer | loading train data for epoch 77
2022-10-14 13:12:53 | INFO | fairseq.data.data_utils | loaded 7096 examples from: data-bin/phoenix2014T/sp25000/train.sign-de.de
2022-10-14 13:12:53 | INFO | fairseq.tasks.translation_sign | data-bin/phoenix2014T/sp25000 train sign-de 7096 examples
/home/tangli/anaconda3/envs/TSPNet/lib/python3.6/site-packages/torch/nn/parallel/distributed.py:553: UserWarning: The `check_reduction` argument in `DistributedDataParallel` module is deprecated. Please avoid using it.
  "The `check_reduction` argument in `DistributedDataParallel` "
/home/tangli/anaconda3/envs/TSPNet/lib/python3.6/site-packages/torch/nn/parallel/distributed.py:553: UserWarning: The `check_reduction` argument in `DistributedDataParallel` module is deprecated. Please avoid using it.
  "The `check_reduction` argument in `DistributedDataParallel` "
/home/tangli/anaconda3/envs/TSPNet/lib/python3.6/site-packages/torch/nn/parallel/distributed.py:553: UserWarning: The `check_reduction` argument in `DistributedDataParallel` module is deprecated. Please avoid using it.
  "The `check_reduction` argument in `DistributedDataParallel` "
/home/tangli/anaconda3/envs/TSPNet/lib/python3.6/site-packages/torch/nn/parallel/distributed.py:553: UserWarning: The `check_reduction` argument in `DistributedDataParallel` module is deprecated. Please avoid using it.
  "The `check_reduction` argument in `DistributedDataParallel` "
2022-10-14 13:12:56 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 10.76 GiB total capacity; 446.60 MiB already allocated; 19.38 MiB free; 456.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-10-14 13:12:56 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 1         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  457314 KB |  457330 KB |  557686 KB |  100372 KB |
|       from large pool |  418208 KB |  418208 KB |  518560 KB |  100352 KB |
|       from small pool |   39106 KB |   39122 KB |   39126 KB |      20 KB |
|---------------------------------------------------------------------------|
| Active memory         |  457314 KB |  457330 KB |  557686 KB |  100372 KB |
|       from large pool |  418208 KB |  418208 KB |  518560 KB |  100352 KB |
|       from small pool |   39106 KB |   39122 KB |   39126 KB |      20 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  466944 KB |  466944 KB |  466944 KB |       0 B  |
|       from large pool |  421888 KB |  421888 KB |  421888 KB |       0 B  |
|       from small pool |   45056 KB |   45056 KB |   45056 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |    9629 KB |  107119 KB |  298020 KB |  288391 KB |
|       from large pool |    3680 KB |  103636 KB |  258293 KB |  254613 KB |
|       from small pool |    5949 KB |    7098 KB |   39727 KB |   33777 KB |
|---------------------------------------------------------------------------|
| Allocations           |     551    |     552    |     555    |       4    |
|       from large pool |     110    |     110    |     111    |       1    |
|       from small pool |     441    |     442    |     444    |       3    |
|---------------------------------------------------------------------------|
| Active allocs         |     551    |     552    |     555    |       4    |
|       from large pool |     110    |     110    |     111    |       1    |
|       from small pool |     441    |     442    |     444    |       3    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      38    |      38    |      38    |       0    |
|       from large pool |      16    |      16    |      16    |       0    |
|       from small pool |      22    |      22    |      22    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      23    |      23    |      35    |      12    |
|       from large pool |       1    |       4    |      11    |      10    |
|       from small pool |      22    |      22    |      24    |       2    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-10-14 13:12:56 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-10-14 13:12:56 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-10-14 13:12:56 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-10-14 13:12:56 | WARNING | fairseq.trainer | ran out of memory in validation step, retrying batch
2022-10-14 13:12:56 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 10.76 GiB total capacity; 446.61 MiB already allocated; 19.38 MiB free; 456.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-10-14 13:12:56 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 2            |        cudaMalloc retries: 2         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  457330 KB |  457345 KB |  557717 KB |  100387 KB |
|       from large pool |  418208 KB |  418208 KB |  518560 KB |  100352 KB |
|       from small pool |   39122 KB |   39137 KB |   39157 KB |      35 KB |
|---------------------------------------------------------------------------|
| Active memory         |  457330 KB |  457345 KB |  557717 KB |  100387 KB |
|       from large pool |  418208 KB |  418208 KB |  518560 KB |  100352 KB |
|       from small pool |   39122 KB |   39137 KB |   39157 KB |      35 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  466944 KB |  466944 KB |  466944 KB |       0 B  |
|       from large pool |  421888 KB |  421888 KB |  421888 KB |       0 B  |
|       from small pool |   45056 KB |   45056 KB |   45056 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |    9614 KB |  107119 KB |  298036 KB |  288422 KB |
|       from large pool |    3680 KB |  103636 KB |  258293 KB |  254613 KB |
|       from small pool |    5934 KB |    7098 KB |   39742 KB |   33808 KB |
|---------------------------------------------------------------------------|
| Allocations           |     552    |     553    |     557    |       5    |
|       from large pool |     110    |     110    |     111    |       1    |
|       from small pool |     442    |     443    |     446    |       4    |
|---------------------------------------------------------------------------|
| Active allocs         |     552    |     553    |     557    |       5    |
|       from large pool |     110    |     110    |     111    |       1    |
|       from small pool |     442    |     443    |     446    |       4    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      38    |      38    |      38    |       0    |
|       from large pool |      16    |      16    |      16    |       0    |
|       from small pool |      22    |      22    |      22    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      23    |      23    |      36    |      13    |
|       from large pool |       1    |       4    |      11    |      10    |
|       from small pool |      22    |      22    |      25    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-10-14 13:12:56 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-10-14 13:12:56 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-10-14 13:12:56 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [0].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [52].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [52].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [52], which does not match the required output shape [68].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [52], which does not match the required output shape [68].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [68], which does not match the required output shape [107].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [68], which does not match the required output shape [107].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [96, 20], which does not match the required output shape [95, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [96, 10], which does not match the required output shape [95, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [96, 10], which does not match the required output shape [95, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [96, 20], which does not match the required output shape [95, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [96, 20], which does not match the required output shape [95, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [107], which does not match the required output shape [118].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [107], which does not match the required output shape [118].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [95, 20], which does not match the required output shape [90, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [95, 10], which does not match the required output shape [90, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [95, 10], which does not match the required output shape [90, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [95, 20], which does not match the required output shape [90, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [95, 20], which does not match the required output shape [90, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [118], which does not match the required output shape [139].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [118], which does not match the required output shape [139].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [90, 20], which does not match the required output shape [80, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [90, 10], which does not match the required output shape [80, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [90, 10], which does not match the required output shape [80, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [90, 20], which does not match the required output shape [80, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [90, 20], which does not match the required output shape [80, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [139], which does not match the required output shape [124].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [139], which does not match the required output shape [124].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [80, 20], which does not match the required output shape [60, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [80, 10], which does not match the required output shape [60, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [80, 10], which does not match the required output shape [60, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [80, 20], which does not match the required output shape [60, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [80, 20], which does not match the required output shape [60, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [124], which does not match the required output shape [114].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [124], which does not match the required output shape [114].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [60, 20], which does not match the required output shape [49, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [60, 10], which does not match the required output shape [49, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [60, 10], which does not match the required output shape [49, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [60, 20], which does not match the required output shape [49, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [60, 20], which does not match the required output shape [49, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [114], which does not match the required output shape [92].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [114], which does not match the required output shape [92].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [49, 20], which does not match the required output shape [38, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [49, 10], which does not match the required output shape [38, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [49, 10], which does not match the required output shape [38, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [49, 20], which does not match the required output shape [38, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [49, 20], which does not match the required output shape [38, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [92], which does not match the required output shape [103].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [92], which does not match the required output shape [103].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [38, 20], which does not match the required output shape [28, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [38, 10], which does not match the required output shape [28, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [38, 10], which does not match the required output shape [28, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [38, 20], which does not match the required output shape [28, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [38, 20], which does not match the required output shape [28, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [103], which does not match the required output shape [68].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [103], which does not match the required output shape [68].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [28, 20], which does not match the required output shape [17, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [28, 10], which does not match the required output shape [17, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [28, 10], which does not match the required output shape [17, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [28, 20], which does not match the required output shape [17, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [28, 20], which does not match the required output shape [17, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [68], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [68], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [17, 20], which does not match the required output shape [13, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [17, 10], which does not match the required output shape [13, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [17, 10], which does not match the required output shape [13, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [17, 20], which does not match the required output shape [13, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [17, 20], which does not match the required output shape [13, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [43].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [43].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [13, 20], which does not match the required output shape [6, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [13, 10], which does not match the required output shape [6, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [13, 10], which does not match the required output shape [6, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [13, 20], which does not match the required output shape [6, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [13, 20], which does not match the required output shape [6, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [43], which does not match the required output shape [38].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [43], which does not match the required output shape [38].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [6, 20], which does not match the required output shape [2, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
2022-10-14 13:12:57 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 152.00 MiB (GPU 3; 10.76 GiB total capacity; 1.30 GiB already allocated; 153.56 MiB free; 1.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-10-14 13:12:57 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-10-14 13:12:57 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-10-14 13:12:57 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-10-14 13:12:57 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 1         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1327 MB |    1372 MB |    5456 MB |    4129 MB |
|       from large pool |    1276 MB |    1320 MB |    5320 MB |    4043 MB |
|       from small pool |      51 MB |      59 MB |     136 MB |      85 MB |
|---------------------------------------------------------------------------|
| Active memory         |    1327 MB |    1372 MB |    5456 MB |    4129 MB |
|       from large pool |    1276 MB |    1320 MB |    5320 MB |    4043 MB |
|       from small pool |      51 MB |      59 MB |     136 MB |      85 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1408 MB |    1514 MB |    1514 MB |  108544 KB |
|       from large pool |    1350 MB |    1446 MB |    1446 MB |   98304 KB |
|       from small pool |      58 MB |      68 MB |      68 MB |   10240 KB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   82043 KB |  112719 KB |    1726 MB |    1646 MB |
|       from large pool |   75591 KB |  104604 KB |    1584 MB |    1510 MB |
|       from small pool |    6452 KB |   13872 KB |     142 MB |     136 MB |
|---------------------------------------------------------------------------|
| Allocations           |     607    |     608    |    1240    |     633    |
|       from large pool |     127    |     128    |     420    |     293    |
|       from small pool |     480    |     483    |     820    |     340    |
|---------------------------------------------------------------------------|
| Active allocs         |     607    |     608    |    1240    |     633    |
|       from large pool |     127    |     128    |     420    |     293    |
|       from small pool |     480    |     483    |     820    |     340    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      58    |      69    |      69    |      11    |
|       from large pool |      29    |      35    |      35    |       6    |
|       from small pool |      29    |      34    |      34    |       5    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      41    |      47    |     327    |     286    |
|       from large pool |      12    |      13    |     165    |     153    |
|       from small pool |      29    |      36    |     162    |     133    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-10-14 13:12:57 | WARNING | fairseq.trainer | ran out of memory in validation step, retrying batch
2022-10-14 13:12:57 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 3; 10.76 GiB total capacity; 1.45 GiB already allocated; 29.56 MiB free; 1.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-10-14 13:12:57 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-10-14 13:12:57 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-10-14 13:12:57 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-10-14 13:12:57 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 2            |        cudaMalloc retries: 2         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1436 MB |    1482 MB |    5689 MB |    4253 MB |
|       from large pool |    1384 MB |    1430 MB |    5551 MB |    4167 MB |
|       from small pool |      51 MB |      59 MB |     137 MB |      85 MB |
|---------------------------------------------------------------------------|
| Active memory         |    1436 MB |    1482 MB |    5689 MB |    4253 MB |
|       from large pool |    1384 MB |    1430 MB |    5551 MB |    4167 MB |
|       from small pool |      51 MB |      59 MB |     137 MB |      85 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1532 MB |    1548 MB |    1654 MB |  124928 KB |
|       from large pool |    1474 MB |    1490 MB |    1586 MB |  114688 KB |
|       from small pool |      58 MB |      68 MB |      68 MB |   10240 KB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   50694 KB |  112719 KB |    1772 MB |    1723 MB |
|       from large pool |   44487 KB |  104604 KB |    1629 MB |    1586 MB |
|       from small pool |    6207 KB |   13872 KB |     143 MB |     137 MB |
|---------------------------------------------------------------------------|
| Allocations           |     617    |     618    |    1286    |     669    |
|       from large pool |     132    |     133    |     431    |     299    |
|       from small pool |     485    |     485    |     855    |     370    |
|---------------------------------------------------------------------------|
| Active allocs         |     617    |     618    |    1286    |     669    |
|       from large pool |     132    |     133    |     431    |     299    |
|       from small pool |     485    |     485    |     855    |     370    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      62    |      69    |      74    |      12    |
|       from large pool |      33    |      35    |      40    |       7    |
|       from small pool |      29    |      34    |      34    |       5    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      43    |      47    |     340    |     297    |
|       from large pool |      13    |      13    |     168    |     155    |
|       from small pool |      30    |      36    |     172    |     142    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [6, 10], which does not match the required output shape [2, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [6, 10], which does not match the required output shape [2, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [6, 20], which does not match the required output shape [2, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [6, 20], which does not match the required output shape [2, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [2, 20], which does not match the required output shape [1, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [2, 10], which does not match the required output shape [1, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [2, 10], which does not match the required output shape [1, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [2, 20], which does not match the required output shape [1, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [2, 20], which does not match the required output shape [1, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
2022-10-14 13:12:57 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 2; 10.76 GiB total capacity; 748.11 MiB already allocated; 45.56 MiB free; 758.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-10-14 13:12:57 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-10-14 13:12:57 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-10-14 13:12:57 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 2         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  766069 KB |  766069 KB |    1064 MB |  324211 KB |
|       from large pool |  726182 KB |  726182 KB |    1025 MB |  323584 KB |
|       from small pool |   39887 KB |   39887 KB |      39 MB |     627 KB |
|---------------------------------------------------------------------------|
| Active memory         |  766069 KB |  766069 KB |    1064 MB |  324211 KB |
|       from large pool |  726182 KB |  726182 KB |    1025 MB |  323584 KB |
|       from small pool |   39887 KB |   39887 KB |      39 MB |     627 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  776192 KB |     782 MB |     836 MB |   79872 KB |
|       from large pool |  731136 KB |     738 MB |     792 MB |   79872 KB |
|       from small pool |   45056 KB |      44 MB |      44 MB |       0 KB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10122 KB |  107119 KB |  299901 KB |  289778 KB |
|       from large pool |    4953 KB |  103636 KB |  259567 KB |  254613 KB |
|       from small pool |    5169 KB |    7098 KB |   40334 KB |   35165 KB |
|---------------------------------------------------------------------------|
| Allocations           |     565    |     565    |     606    |      41    |
|       from large pool |     119    |     119    |     128    |       9    |
|       from small pool |     446    |     446    |     478    |      32    |
|---------------------------------------------------------------------------|
| Active allocs         |     565    |     565    |     606    |      41    |
|       from large pool |     119    |     119    |     128    |       9    |
|       from small pool |     446    |     446    |     478    |      32    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      47    |      49    |      50    |       3    |
|       from large pool |      25    |      27    |      28    |       3    |
|       from small pool |      22    |      22    |      22    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      24    |      24    |      45    |      21    |
|       from large pool |       2    |       4    |      12    |      10    |
|       from small pool |      22    |      23    |      33    |      11    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-10-14 13:12:57 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-10-14 13:12:57 | WARNING | fairseq.trainer | ran out of memory in validation step, retrying batch
2022-10-14 13:12:57 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 2; 10.76 GiB total capacity; 780.13 MiB already allocated; 13.56 MiB free; 790.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-10-14 13:12:57 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-10-14 13:12:57 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-10-14 13:12:57 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 2            |        cudaMalloc retries: 3         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  782469 KB |     780 MB |    1096 MB |  340785 KB |
|       from large pool |  742566 KB |     741 MB |    1057 MB |  339968 KB |
|       from small pool |   39902 KB |      39 MB |      39 MB |     817 KB |
|---------------------------------------------------------------------------|
| Active memory         |  782469 KB |     780 MB |    1096 MB |  340785 KB |
|       from large pool |  742566 KB |     741 MB |    1057 MB |  339968 KB |
|       from small pool |   39902 KB |      39 MB |      39 MB |     817 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |     790 MB |     790 MB |     868 MB |   79872 KB |
|       from large pool |     746 MB |     746 MB |     824 MB |   79872 KB |
|       from small pool |      44 MB |      44 MB |      44 MB |       0 KB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10107 KB |  107119 KB |  300091 KB |  289984 KB |
|       from large pool |    4953 KB |  103636 KB |  259567 KB |  254613 KB |
|       from small pool |    5153 KB |    7098 KB |   40524 KB |   35370 KB |
|---------------------------------------------------------------------------|
| Allocations           |     567    |     570    |     618    |      51    |
|       from large pool |     120    |     121    |     130    |      10    |
|       from small pool |     447    |     450    |     488    |      41    |
|---------------------------------------------------------------------------|
| Active allocs         |     567    |     570    |     618    |      51    |
|       from large pool |     120    |     121    |     130    |      10    |
|       from small pool |     447    |     450    |     488    |      41    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      49    |      49    |      52    |       3    |
|       from large pool |      27    |      27    |      30    |       3    |
|       from small pool |      22    |      22    |      22    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      25    |      25    |      48    |      23    |
|       from large pool |       2    |       4    |      12    |      10    |
|       from small pool |      23    |      23    |      36    |      13    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-10-14 13:12:57 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

Traceback (most recent call last):
  File "test_scripts/test_sign_local.py", line 244, in <module>
    cli_main()
  File "test_scripts/test_sign_local.py", line 236, in cli_main
    nprocs=args.distributed_world_size,
  File "/home/tangli/anaconda3/envs/TSPNet/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/tangli/anaconda3/envs/TSPNet/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 188, in start_processes
    while not context.join():
  File "/home/tangli/anaconda3/envs/TSPNet/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 150, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/home/tangli/TSPNet/fairseq/trainer.py", line 437, in valid_step
    sample, self.model, self.criterion
  File "/home/tangli/TSPNet/fairseq/tasks/translation_sign.py", line 237, in valid_step
    loss, sample_size, logging_output = super().valid_step(sample, model, criterion)
  File "/home/tangli/TSPNet/fairseq/tasks/fairseq_task.py", line 304, in valid_step
    loss, sample_size, logging_output = criterion(model, sample)
  File "/home/tangli/anaconda3/envs/TSPNet/lib/python3.6/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/tangli/TSPNet/fairseq/criterions/label_smoothed_cross_entropy.py", line 56, in forward
    net_output = model(**sample['net_input'])
  File "/home/tangli/anaconda3/envs/TSPNet/lib/python3.6/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/tangli/anaconda3/envs/TSPNet/lib/python3.6/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/home/tangli/anaconda3/envs/TSPNet/lib/python3.6/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/tangli/TSPNet/fairseq/models/fairseq_model.py", line 227, in forward
    encoder_out = self.encoder(src_tokens, src_lengths=src_lengths, **kwargs)
  File "/home/tangli/anaconda3/envs/TSPNet/lib/python3.6/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/tangli/TSPNet/fairseq/models/transformer_from_sign.py", line 352, in forward
    x_lv, encoder_embedding_lv = self.forward_embedding(src_embed=src_token_lv, pseudo_src_tokens=pseudo_src_tokens_lv)
  File "/home/tangli/TSPNet/fairseq/models/transformer_from_sign.py", line 304, in forward_embedding
    x = embed = self.embed_scale * src_embed
RuntimeError: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 10.76 GiB total capacity; 446.60 MiB already allocated; 19.38 MiB free; 456.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/tangli/anaconda3/envs/TSPNet/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 59, in _wrap
    fn(i, *args)
  File "/home/tangli/TSPNet/test_scripts/test_sign_local.py", line 174, in distributed_main
    main(args, init_distributed=True)
  File "/home/tangli/TSPNet/test_scripts/test_sign_local.py", line 86, in main
    hyps, refs = validate(args, trainer, task, epoch_itr, valid_subsets)
  File "/home/tangli/TSPNet/test_scripts/test_sign_local.py", line 141, in validate
    logging_output, h, r = trainer.valid_step(sample, generate=True)
  File "/home/tangli/anaconda3/envs/TSPNet/lib/python3.6/contextlib.py", line 52, in inner
    return func(*args, **kwds)
  File "/home/tangli/TSPNet/fairseq/trainer.py", line 451, in valid_step
    return self.valid_step(sample, raise_oom=True)
  File "/home/tangli/anaconda3/envs/TSPNet/lib/python3.6/contextlib.py", line 52, in inner
    return func(*args, **kwds)
  File "/home/tangli/TSPNet/fairseq/trainer.py", line 452, in valid_step
    raise e
  File "/home/tangli/TSPNet/fairseq/trainer.py", line 437, in valid_step
    sample, self.model, self.criterion
  File "/home/tangli/TSPNet/fairseq/tasks/translation_sign.py", line 237, in valid_step
    loss, sample_size, logging_output = super().valid_step(sample, model, criterion)
  File "/home/tangli/TSPNet/fairseq/tasks/fairseq_task.py", line 304, in valid_step
    loss, sample_size, logging_output = criterion(model, sample)
  File "/home/tangli/anaconda3/envs/TSPNet/lib/python3.6/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/tangli/TSPNet/fairseq/criterions/label_smoothed_cross_entropy.py", line 56, in forward
    net_output = model(**sample['net_input'])
  File "/home/tangli/anaconda3/envs/TSPNet/lib/python3.6/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/tangli/anaconda3/envs/TSPNet/lib/python3.6/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/home/tangli/anaconda3/envs/TSPNet/lib/python3.6/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/tangli/TSPNet/fairseq/models/fairseq_model.py", line 227, in forward
    encoder_out = self.encoder(src_tokens, src_lengths=src_lengths, **kwargs)
  File "/home/tangli/anaconda3/envs/TSPNet/lib/python3.6/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/tangli/TSPNet/fairseq/models/transformer_from_sign.py", line 352, in forward
    x_lv, encoder_embedding_lv = self.forward_embedding(src_embed=src_token_lv, pseudo_src_tokens=pseudo_src_tokens_lv)
  File "/home/tangli/TSPNet/fairseq/models/transformer_from_sign.py", line 304, in forward_embedding
    x = embed = self.embed_scale * src_embed
RuntimeError: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 10.76 GiB total capacity; 446.61 MiB already allocated; 19.38 MiB free; 456.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

2022-10-14 13:17:56 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='gelu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_sign', attention_dropout=0.0, best_checkpoint_metric='bleu', bpe=None, bucket_cap_mb=25, clip_norm=25, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/phoenix2014T/sp25000', dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=10, decoder_embed_dim=300, decoder_embed_path='data-bin/phoenix2014T/sp25000/emb', decoder_ffn_embed_dim=2048, decoder_input_dim=300, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=300, device_id=0, disable_bucketing=False, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.4, early_proj=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=1, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, eval_bleu=True, eval_bleu_args='{"beam": 10, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe='sentencepiece', eval_bleu_save=None, eval_tokenized_bleu=True, fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layer_wise_attention=False, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', log_format=None, log_interval=1000, lr=[0.0001], lr_mode='max', lr_patience=8, lr_schedule_metric='bleu', lr_scheduler='reduce_lr_on_plateau', lr_shrink=0.5, lr_threshold=0.0001, max_epoch=200, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1, multilv_args='{"span_lengths": [8, 12, 16], "level_links": [[1, 0], [2, 1], [2, 0]], "stride": 2, "eye": true, "same_level_links": true, "symmetric": true}', no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_token_positional_embeddings=False, num_levels=3, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='/home/tangli/TSPNet/CHECKPOINT_PATH/checkpoint_best.pt', save_dir='checkpoints', save_interval=200, save_interval_updates=0, seed=1, sentence_avg=False, share_all_embeddings=False, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, src_lv0_body_feat_root='i3d-features/span=8_stride=2', src_lv0_lefthand_feat_root=None, src_lv0_righthand_feat_root=None, src_lv1_body_feat_root='i3d-features/span=12_stride=2', src_lv1_lefthand_feat_root=None, src_lv1_righthand_feat_root=None, src_lv2_body_feat_root='i3d-features/span=16_stride=2', src_lv2_lefthand_feat_root=None, src_lv2_righthand_feat_root=None, target_lang='de', task='translation_sign', tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, train_subset='train', update_freq=[1], use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='test', validate_interval=1, warmup_init_lr=-1, warmup_updates=0, weight_decay=0.0001)
2022-10-14 13:17:56 | INFO | root | [de] dictionary: 2656 types
2022-10-14 13:17:56 | INFO | fairseq.data.data_utils | loaded 642 examples from: data-bin/phoenix2014T/sp25000/test.sign-de.de
2022-10-14 13:17:56 | INFO | fairseq.tasks.translation_sign | data-bin/phoenix2014T/sp25000 test sign-de 642 examples
2022-10-14 13:17:56 | INFO | root | Loaded pretrained embeddings from data-bin/phoenix2014T/sp25000/emb
2022-10-14 13:17:56 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoderSign(
    (embed_positions): LearnedPositionalEmbedding(1025, 1024, padding_idx=0)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (embed_tokens): Embedding(2656, 300, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 300, padding_idx=1)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=300, out_features=300, bias=True)
          (v_proj): Linear(in_features=300, out_features=300, bias=True)
          (q_proj): Linear(in_features=300, out_features=300, bias=True)
          (out_proj): Linear(in_features=300, out_features=300, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=1024, out_features=300, bias=True)
          (v_proj): Linear(in_features=1024, out_features=300, bias=True)
          (q_proj): Linear(in_features=300, out_features=300, bias=True)
          (out_proj): Linear(in_features=300, out_features=300, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=300, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=300, bias=True)
        (final_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=300, out_features=300, bias=True)
          (v_proj): Linear(in_features=300, out_features=300, bias=True)
          (q_proj): Linear(in_features=300, out_features=300, bias=True)
          (out_proj): Linear(in_features=300, out_features=300, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=1024, out_features=300, bias=True)
          (v_proj): Linear(in_features=1024, out_features=300, bias=True)
          (q_proj): Linear(in_features=300, out_features=300, bias=True)
          (out_proj): Linear(in_features=300, out_features=300, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=300, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=300, bias=True)
        (final_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=300, out_features=300, bias=True)
          (v_proj): Linear(in_features=300, out_features=300, bias=True)
          (q_proj): Linear(in_features=300, out_features=300, bias=True)
          (out_proj): Linear(in_features=300, out_features=300, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=1024, out_features=300, bias=True)
          (v_proj): Linear(in_features=1024, out_features=300, bias=True)
          (q_proj): Linear(in_features=300, out_features=300, bias=True)
          (out_proj): Linear(in_features=300, out_features=300, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=300, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=300, bias=True)
        (final_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=300, out_features=300, bias=True)
          (v_proj): Linear(in_features=300, out_features=300, bias=True)
          (q_proj): Linear(in_features=300, out_features=300, bias=True)
          (out_proj): Linear(in_features=300, out_features=300, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=1024, out_features=300, bias=True)
          (v_proj): Linear(in_features=1024, out_features=300, bias=True)
          (q_proj): Linear(in_features=300, out_features=300, bias=True)
          (out_proj): Linear(in_features=300, out_features=300, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=300, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=300, bias=True)
        (final_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=300, out_features=300, bias=True)
          (v_proj): Linear(in_features=300, out_features=300, bias=True)
          (q_proj): Linear(in_features=300, out_features=300, bias=True)
          (out_proj): Linear(in_features=300, out_features=300, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=1024, out_features=300, bias=True)
          (v_proj): Linear(in_features=1024, out_features=300, bias=True)
          (q_proj): Linear(in_features=300, out_features=300, bias=True)
          (out_proj): Linear(in_features=300, out_features=300, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=300, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=300, bias=True)
        (final_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=300, out_features=300, bias=True)
          (v_proj): Linear(in_features=300, out_features=300, bias=True)
          (q_proj): Linear(in_features=300, out_features=300, bias=True)
          (out_proj): Linear(in_features=300, out_features=300, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=1024, out_features=300, bias=True)
          (v_proj): Linear(in_features=1024, out_features=300, bias=True)
          (q_proj): Linear(in_features=300, out_features=300, bias=True)
          (out_proj): Linear(in_features=300, out_features=300, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=300, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=300, bias=True)
        (final_layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
)
2022-10-14 13:17:56 | INFO | fairseq_cli.train | model transformer_sign, criterion LabelSmoothedCrossEntropyCriterion
2022-10-14 13:17:56 | INFO | fairseq_cli.train | num. model params: 25689360 (num. trained: 25689360)
2022-10-14 13:18:00 | INFO | fairseq_cli.train | training on 1 GPUs
2022-10-14 13:18:00 | INFO | fairseq_cli.train | max tokens per GPU = 4096 and max sentences per GPU = None
2022-10-14 13:18:01 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16
2022-10-14 13:18:01 | INFO | fairseq.trainer | loaded checkpoint /home/tangli/TSPNet/CHECKPOINT_PATH/checkpoint_best.pt (epoch 77 @ 8547 updates)
2022-10-14 13:18:01 | INFO | fairseq.trainer | loading train data for epoch 77
2022-10-14 13:18:02 | INFO | fairseq.data.data_utils | loaded 7096 examples from: data-bin/phoenix2014T/sp25000/train.sign-de.de
2022-10-14 13:18:02 | INFO | fairseq.tasks.translation_sign | data-bin/phoenix2014T/sp25000 train sign-de 7096 examples
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [33].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [33].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [109].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [109].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [109], which does not match the required output shape [139].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [109], which does not match the required output shape [139].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [128, 20], which does not match the required output shape [127, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [128, 10], which does not match the required output shape [127, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [128, 10], which does not match the required output shape [127, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [128, 20], which does not match the required output shape [127, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [128, 20], which does not match the required output shape [127, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [139], which does not match the required output shape [172].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [139], which does not match the required output shape [172].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [127, 20], which does not match the required output shape [117, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [127, 10], which does not match the required output shape [117, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [127, 10], which does not match the required output shape [117, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [127, 20], which does not match the required output shape [117, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [127, 20], which does not match the required output shape [117, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [172], which does not match the required output shape [177].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [172], which does not match the required output shape [177].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [117, 20], which does not match the required output shape [107, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [117, 10], which does not match the required output shape [107, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [117, 10], which does not match the required output shape [107, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [117, 20], which does not match the required output shape [107, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [117, 20], which does not match the required output shape [107, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [177], which does not match the required output shape [238].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [177], which does not match the required output shape [238].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [107, 20], which does not match the required output shape [82, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [107, 10], which does not match the required output shape [82, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [107, 10], which does not match the required output shape [82, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [107, 20], which does not match the required output shape [82, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [107, 20], which does not match the required output shape [82, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [238], which does not match the required output shape [197].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [238], which does not match the required output shape [197].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [82, 20], which does not match the required output shape [60, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [82, 10], which does not match the required output shape [60, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [82, 10], which does not match the required output shape [60, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [82, 20], which does not match the required output shape [60, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [82, 20], which does not match the required output shape [60, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [197], which does not match the required output shape [148].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [197], which does not match the required output shape [148].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [60, 20], which does not match the required output shape [39, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [60, 10], which does not match the required output shape [39, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [60, 10], which does not match the required output shape [39, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [60, 20], which does not match the required output shape [39, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [60, 20], which does not match the required output shape [39, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [148], which does not match the required output shape [87].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [148], which does not match the required output shape [87].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [39, 20], which does not match the required output shape [27, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [39, 10], which does not match the required output shape [27, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [39, 10], which does not match the required output shape [27, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [39, 20], which does not match the required output shape [27, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [39, 20], which does not match the required output shape [27, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [87], which does not match the required output shape [66].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [87], which does not match the required output shape [66].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [27, 20], which does not match the required output shape [17, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [27, 10], which does not match the required output shape [17, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [27, 10], which does not match the required output shape [17, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [27, 20], which does not match the required output shape [17, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [27, 20], which does not match the required output shape [17, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [66], which does not match the required output shape [31].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [66], which does not match the required output shape [31].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [17, 20], which does not match the required output shape [10, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [17, 10], which does not match the required output shape [10, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [17, 10], which does not match the required output shape [10, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [17, 20], which does not match the required output shape [10, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [17, 20], which does not match the required output shape [10, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [10, 20], which does not match the required output shape [7, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [10, 10], which does not match the required output shape [7, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [10, 10], which does not match the required output shape [7, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [10, 20], which does not match the required output shape [7, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [10, 20], which does not match the required output shape [7, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [7, 20], which does not match the required output shape [5, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [7, 10], which does not match the required output shape [5, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [7, 10], which does not match the required output shape [5, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [7, 20], which does not match the required output shape [5, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [7, 20], which does not match the required output shape [5, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [5, 20], which does not match the required output shape [3, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [5, 10], which does not match the required output shape [3, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [5, 10], which does not match the required output shape [3, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [5, 20], which does not match the required output shape [3, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [5, 20], which does not match the required output shape [3, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [3, 20], which does not match the required output shape [1, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [3, 10], which does not match the required output shape [1, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [3, 10], which does not match the required output shape [1, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [3, 20], which does not match the required output shape [1, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [3, 20], which does not match the required output shape [1, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [1, 20], which does not match the required output shape [96, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [1, 20], which does not match the required output shape [96, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [0].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [52].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [52].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [52], which does not match the required output shape [68].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [52], which does not match the required output shape [68].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [68], which does not match the required output shape [107].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [68], which does not match the required output shape [107].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [96, 20], which does not match the required output shape [95, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [96, 10], which does not match the required output shape [95, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [96, 10], which does not match the required output shape [95, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [96, 20], which does not match the required output shape [95, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [96, 20], which does not match the required output shape [95, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [107], which does not match the required output shape [118].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [107], which does not match the required output shape [118].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [95, 20], which does not match the required output shape [90, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [95, 10], which does not match the required output shape [90, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [95, 10], which does not match the required output shape [90, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [95, 20], which does not match the required output shape [90, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [95, 20], which does not match the required output shape [90, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [118], which does not match the required output shape [139].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [118], which does not match the required output shape [139].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [90, 20], which does not match the required output shape [80, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [90, 10], which does not match the required output shape [80, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [90, 10], which does not match the required output shape [80, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [90, 20], which does not match the required output shape [80, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [90, 20], which does not match the required output shape [80, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [139], which does not match the required output shape [124].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [139], which does not match the required output shape [124].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [80, 20], which does not match the required output shape [60, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [80, 10], which does not match the required output shape [60, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [80, 10], which does not match the required output shape [60, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [80, 20], which does not match the required output shape [60, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [80, 20], which does not match the required output shape [60, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [124], which does not match the required output shape [114].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [124], which does not match the required output shape [114].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [60, 20], which does not match the required output shape [49, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [60, 10], which does not match the required output shape [49, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [60, 10], which does not match the required output shape [49, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [60, 20], which does not match the required output shape [49, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [60, 20], which does not match the required output shape [49, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [114], which does not match the required output shape [92].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [114], which does not match the required output shape [92].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [49, 20], which does not match the required output shape [38, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [49, 10], which does not match the required output shape [38, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [49, 10], which does not match the required output shape [38, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [49, 20], which does not match the required output shape [38, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [49, 20], which does not match the required output shape [38, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [92], which does not match the required output shape [103].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [92], which does not match the required output shape [103].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [38, 20], which does not match the required output shape [28, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [38, 10], which does not match the required output shape [28, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [38, 10], which does not match the required output shape [28, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [38, 20], which does not match the required output shape [28, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [38, 20], which does not match the required output shape [28, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [103], which does not match the required output shape [68].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [103], which does not match the required output shape [68].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [28, 20], which does not match the required output shape [17, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [28, 10], which does not match the required output shape [17, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [28, 10], which does not match the required output shape [17, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [28, 20], which does not match the required output shape [17, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [28, 20], which does not match the required output shape [17, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [68], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [68], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [17, 20], which does not match the required output shape [13, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [17, 10], which does not match the required output shape [13, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [17, 10], which does not match the required output shape [13, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [17, 20], which does not match the required output shape [13, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [17, 20], which does not match the required output shape [13, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [43].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [43].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [13, 20], which does not match the required output shape [6, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [13, 10], which does not match the required output shape [6, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [13, 10], which does not match the required output shape [6, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [13, 20], which does not match the required output shape [6, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [13, 20], which does not match the required output shape [6, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [43], which does not match the required output shape [38].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [43], which does not match the required output shape [38].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [6, 20], which does not match the required output shape [2, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [6, 10], which does not match the required output shape [2, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [6, 10], which does not match the required output shape [2, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [6, 20], which does not match the required output shape [2, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [6, 20], which does not match the required output shape [2, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [2, 20], which does not match the required output shape [1, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [2, 10], which does not match the required output shape [1, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [2, 10], which does not match the required output shape [1, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [2, 20], which does not match the required output shape [1, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [2, 20], which does not match the required output shape [1, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [1, 20], which does not match the required output shape [80, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [1, 20], which does not match the required output shape [80, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [0].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [80, 20], which does not match the required output shape [79, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [80, 10], which does not match the required output shape [79, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [80, 10], which does not match the required output shape [79, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [80, 20], which does not match the required output shape [79, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [80, 20], which does not match the required output shape [79, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [25].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [25].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [41].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [41].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [79, 20], which does not match the required output shape [78, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [79, 10], which does not match the required output shape [78, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [79, 10], which does not match the required output shape [78, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [79, 20], which does not match the required output shape [78, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [79, 20], which does not match the required output shape [78, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [41], which does not match the required output shape [59].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [41], which does not match the required output shape [59].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [78, 20], which does not match the required output shape [74, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [78, 10], which does not match the required output shape [74, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [78, 10], which does not match the required output shape [74, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [78, 20], which does not match the required output shape [74, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [78, 20], which does not match the required output shape [74, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [59], which does not match the required output shape [73].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [59], which does not match the required output shape [73].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [74, 20], which does not match the required output shape [72, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [74, 10], which does not match the required output shape [72, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [74, 10], which does not match the required output shape [72, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [74, 20], which does not match the required output shape [72, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [74, 20], which does not match the required output shape [72, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [73], which does not match the required output shape [104].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [73], which does not match the required output shape [104].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [72, 20], which does not match the required output shape [63, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [72, 10], which does not match the required output shape [63, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [72, 10], which does not match the required output shape [63, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [72, 20], which does not match the required output shape [63, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [72, 20], which does not match the required output shape [63, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [104], which does not match the required output shape [130].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [104], which does not match the required output shape [130].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [63, 20], which does not match the required output shape [53, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [63, 10], which does not match the required output shape [53, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [63, 10], which does not match the required output shape [53, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [63, 20], which does not match the required output shape [53, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [63, 20], which does not match the required output shape [53, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [130], which does not match the required output shape [80].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [130], which does not match the required output shape [80].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [53, 20], which does not match the required output shape [46, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [53, 10], which does not match the required output shape [46, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [53, 10], which does not match the required output shape [46, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [53, 20], which does not match the required output shape [46, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [53, 20], which does not match the required output shape [46, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [80], which does not match the required output shape [61].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [80], which does not match the required output shape [61].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [46, 20], which does not match the required output shape [37, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [46, 10], which does not match the required output shape [37, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [46, 10], which does not match the required output shape [37, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [46, 20], which does not match the required output shape [37, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [46, 20], which does not match the required output shape [37, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [61], which does not match the required output shape [114].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [61], which does not match the required output shape [114].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [37, 20], which does not match the required output shape [27, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [37, 10], which does not match the required output shape [27, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [37, 10], which does not match the required output shape [27, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [37, 20], which does not match the required output shape [27, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [37, 20], which does not match the required output shape [27, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [114], which does not match the required output shape [108].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [114], which does not match the required output shape [108].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [27, 20], which does not match the required output shape [15, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [27, 10], which does not match the required output shape [15, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [27, 10], which does not match the required output shape [15, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [27, 20], which does not match the required output shape [15, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [27, 20], which does not match the required output shape [15, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [108], which does not match the required output shape [28].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [108], which does not match the required output shape [28].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [15, 20], which does not match the required output shape [9, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [15, 10], which does not match the required output shape [9, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [15, 10], which does not match the required output shape [9, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [15, 20], which does not match the required output shape [9, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [15, 20], which does not match the required output shape [9, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [9, 20], which does not match the required output shape [6, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [9, 10], which does not match the required output shape [6, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [9, 10], which does not match the required output shape [6, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [9, 20], which does not match the required output shape [6, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [9, 20], which does not match the required output shape [6, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [6, 20], which does not match the required output shape [5, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [6, 10], which does not match the required output shape [5, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [6, 10], which does not match the required output shape [5, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [6, 20], which does not match the required output shape [5, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [6, 20], which does not match the required output shape [5, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [5, 20], which does not match the required output shape [4, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [5, 10], which does not match the required output shape [4, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [5, 10], which does not match the required output shape [4, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [5, 20], which does not match the required output shape [4, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [5, 20], which does not match the required output shape [4, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [4, 20], which does not match the required output shape [2, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [4, 10], which does not match the required output shape [2, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [4, 10], which does not match the required output shape [2, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [4, 20], which does not match the required output shape [2, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [4, 20], which does not match the required output shape [2, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [1, 20], which does not match the required output shape [72, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [1, 20], which does not match the required output shape [72, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [0].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [25].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [25].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [46].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [46].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [72, 20], which does not match the required output shape [71, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [72, 10], which does not match the required output shape [71, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [72, 10], which does not match the required output shape [71, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [72, 20], which does not match the required output shape [71, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [72, 20], which does not match the required output shape [71, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [46], which does not match the required output shape [60].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [46], which does not match the required output shape [60].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [71, 20], which does not match the required output shape [66, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [71, 10], which does not match the required output shape [66, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [71, 10], which does not match the required output shape [66, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [71, 20], which does not match the required output shape [66, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [71, 20], which does not match the required output shape [66, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [60], which does not match the required output shape [83].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [60], which does not match the required output shape [83].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [66, 20], which does not match the required output shape [61, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [66, 10], which does not match the required output shape [61, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [66, 10], which does not match the required output shape [61, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [66, 20], which does not match the required output shape [61, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [66, 20], which does not match the required output shape [61, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [83], which does not match the required output shape [103].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [83], which does not match the required output shape [103].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [61, 20], which does not match the required output shape [56, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [61, 10], which does not match the required output shape [56, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [61, 10], which does not match the required output shape [56, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [61, 20], which does not match the required output shape [56, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [61, 20], which does not match the required output shape [56, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [103], which does not match the required output shape [61].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [103], which does not match the required output shape [61].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [56, 20], which does not match the required output shape [49, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [56, 10], which does not match the required output shape [49, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [56, 10], which does not match the required output shape [49, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [56, 20], which does not match the required output shape [49, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [56, 20], which does not match the required output shape [49, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [61], which does not match the required output shape [119].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [61], which does not match the required output shape [119].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [49, 20], which does not match the required output shape [42, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [49, 10], which does not match the required output shape [42, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [49, 10], which does not match the required output shape [42, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [49, 20], which does not match the required output shape [42, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [49, 20], which does not match the required output shape [42, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [119], which does not match the required output shape [123].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [119], which does not match the required output shape [123].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [42, 20], which does not match the required output shape [27, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [42, 10], which does not match the required output shape [27, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [42, 10], which does not match the required output shape [27, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [42, 20], which does not match the required output shape [27, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [42, 20], which does not match the required output shape [27, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [123], which does not match the required output shape [37].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [123], which does not match the required output shape [37].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [27, 20], which does not match the required output shape [21, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [27, 10], which does not match the required output shape [21, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [27, 10], which does not match the required output shape [21, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [27, 20], which does not match the required output shape [21, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [27, 20], which does not match the required output shape [21, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [52].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [52].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [21, 20], which does not match the required output shape [13, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [21, 10], which does not match the required output shape [13, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [21, 10], which does not match the required output shape [13, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [21, 20], which does not match the required output shape [13, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [21, 20], which does not match the required output shape [13, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [52], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [52], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [13, 20], which does not match the required output shape [10, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [13, 10], which does not match the required output shape [10, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [13, 10], which does not match the required output shape [10, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [13, 20], which does not match the required output shape [10, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [13, 20], which does not match the required output shape [10, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [33].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [33].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [10, 20], which does not match the required output shape [5, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [10, 10], which does not match the required output shape [5, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [10, 10], which does not match the required output shape [5, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [10, 20], which does not match the required output shape [5, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [10, 20], which does not match the required output shape [5, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [5, 20], which does not match the required output shape [2, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [5, 10], which does not match the required output shape [2, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [5, 10], which does not match the required output shape [2, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [5, 20], which does not match the required output shape [2, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [5, 20], which does not match the required output shape [2, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [1, 20], which does not match the required output shape [64, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [1, 20], which does not match the required output shape [64, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [0].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [64, 20], which does not match the required output shape [63, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [64, 10], which does not match the required output shape [63, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [64, 10], which does not match the required output shape [63, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [64, 20], which does not match the required output shape [63, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [64, 20], which does not match the required output shape [63, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [31].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [31].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [57].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [57].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [63, 20], which does not match the required output shape [62, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [63, 10], which does not match the required output shape [62, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [63, 10], which does not match the required output shape [62, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [63, 20], which does not match the required output shape [62, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [63, 20], which does not match the required output shape [62, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [57], which does not match the required output shape [67].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [57], which does not match the required output shape [67].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [62, 20], which does not match the required output shape [58, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [62, 10], which does not match the required output shape [58, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [62, 10], which does not match the required output shape [58, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [62, 20], which does not match the required output shape [58, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [62, 20], which does not match the required output shape [58, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [67], which does not match the required output shape [80].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [67], which does not match the required output shape [80].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [58, 20], which does not match the required output shape [51, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [58, 10], which does not match the required output shape [51, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [58, 10], which does not match the required output shape [51, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [58, 20], which does not match the required output shape [51, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [58, 20], which does not match the required output shape [51, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [80], which does not match the required output shape [95].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [80], which does not match the required output shape [95].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [51, 20], which does not match the required output shape [46, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [51, 10], which does not match the required output shape [46, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [51, 10], which does not match the required output shape [46, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [51, 20], which does not match the required output shape [46, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [51, 20], which does not match the required output shape [46, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [89].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [89].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [46, 20], which does not match the required output shape [36, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [46, 10], which does not match the required output shape [36, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [46, 10], which does not match the required output shape [36, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [46, 20], which does not match the required output shape [36, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [46, 20], which does not match the required output shape [36, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [89], which does not match the required output shape [62].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [89], which does not match the required output shape [62].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [36, 20], which does not match the required output shape [29, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [36, 10], which does not match the required output shape [29, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [36, 10], which does not match the required output shape [29, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [36, 20], which does not match the required output shape [29, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [36, 20], which does not match the required output shape [29, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [62], which does not match the required output shape [43].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [62], which does not match the required output shape [43].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [29, 20], which does not match the required output shape [20, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [29, 10], which does not match the required output shape [20, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [29, 10], which does not match the required output shape [20, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [29, 20], which does not match the required output shape [20, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [29, 20], which does not match the required output shape [20, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [43], which does not match the required output shape [25].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [43], which does not match the required output shape [25].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [20, 20], which does not match the required output shape [18, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [20, 10], which does not match the required output shape [18, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [20, 10], which does not match the required output shape [18, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [20, 20], which does not match the required output shape [18, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [20, 20], which does not match the required output shape [18, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [31].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [31].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [18, 20], which does not match the required output shape [14, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [18, 10], which does not match the required output shape [14, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [18, 10], which does not match the required output shape [14, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [18, 20], which does not match the required output shape [14, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [18, 20], which does not match the required output shape [14, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [29].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [29].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [14, 20], which does not match the required output shape [11, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [14, 10], which does not match the required output shape [11, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [14, 10], which does not match the required output shape [11, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [14, 20], which does not match the required output shape [11, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [14, 20], which does not match the required output shape [11, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [11, 20], which does not match the required output shape [7, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [11, 10], which does not match the required output shape [7, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [11, 10], which does not match the required output shape [7, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [11, 20], which does not match the required output shape [7, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [11, 20], which does not match the required output shape [7, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [7, 20], which does not match the required output shape [6, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [7, 10], which does not match the required output shape [6, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [7, 10], which does not match the required output shape [6, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [7, 20], which does not match the required output shape [6, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [7, 20], which does not match the required output shape [6, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [6, 20], which does not match the required output shape [3, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [6, 10], which does not match the required output shape [3, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [6, 10], which does not match the required output shape [3, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [6, 20], which does not match the required output shape [3, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [6, 20], which does not match the required output shape [3, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [3, 20], which does not match the required output shape [2, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [3, 10], which does not match the required output shape [2, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [3, 10], which does not match the required output shape [2, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [3, 20], which does not match the required output shape [2, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [3, 20], which does not match the required output shape [2, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [1, 20], which does not match the required output shape [56, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [1, 20], which does not match the required output shape [56, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [0].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [56, 20], which does not match the required output shape [55, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [56, 10], which does not match the required output shape [55, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [56, 10], which does not match the required output shape [55, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [56, 20], which does not match the required output shape [55, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [56, 20], which does not match the required output shape [55, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [37].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [37].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [55, 20], which does not match the required output shape [53, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [55, 10], which does not match the required output shape [53, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [55, 10], which does not match the required output shape [53, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [55, 20], which does not match the required output shape [53, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [55, 20], which does not match the required output shape [53, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [56].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [56].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [53, 20], which does not match the required output shape [48, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [53, 10], which does not match the required output shape [48, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [53, 10], which does not match the required output shape [48, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [53, 20], which does not match the required output shape [48, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [53, 20], which does not match the required output shape [48, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [56], which does not match the required output shape [46].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [56], which does not match the required output shape [46].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [48, 20], which does not match the required output shape [46, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [48, 10], which does not match the required output shape [46, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [48, 10], which does not match the required output shape [46, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [48, 20], which does not match the required output shape [46, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [48, 20], which does not match the required output shape [46, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [46], which does not match the required output shape [69].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [46], which does not match the required output shape [69].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [46, 20], which does not match the required output shape [43, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [46, 10], which does not match the required output shape [43, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [46, 10], which does not match the required output shape [43, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [46, 20], which does not match the required output shape [43, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [46, 20], which does not match the required output shape [43, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [69], which does not match the required output shape [48].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [69], which does not match the required output shape [48].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [43, 20], which does not match the required output shape [38, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [43, 10], which does not match the required output shape [38, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [43, 10], which does not match the required output shape [38, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [43, 20], which does not match the required output shape [38, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [43, 20], which does not match the required output shape [38, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [38, 20], which does not match the required output shape [34, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [38, 10], which does not match the required output shape [34, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [38, 10], which does not match the required output shape [34, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [38, 20], which does not match the required output shape [34, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [38, 20], which does not match the required output shape [34, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [48], which does not match the required output shape [47].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [48], which does not match the required output shape [47].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [34, 20], which does not match the required output shape [26, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [34, 10], which does not match the required output shape [26, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [34, 10], which does not match the required output shape [26, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [34, 20], which does not match the required output shape [26, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [34, 20], which does not match the required output shape [26, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [47], which does not match the required output shape [36].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [47], which does not match the required output shape [36].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [26, 20], which does not match the required output shape [24, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [26, 10], which does not match the required output shape [24, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [26, 10], which does not match the required output shape [24, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [26, 20], which does not match the required output shape [24, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [26, 20], which does not match the required output shape [24, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [40].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [40].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [24, 20], which does not match the required output shape [18, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [24, 10], which does not match the required output shape [18, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [24, 10], which does not match the required output shape [18, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [24, 20], which does not match the required output shape [18, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [24, 20], which does not match the required output shape [18, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [25].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [25].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [18, 20], which does not match the required output shape [16, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [18, 10], which does not match the required output shape [16, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [18, 10], which does not match the required output shape [16, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [18, 20], which does not match the required output shape [16, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [18, 20], which does not match the required output shape [16, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [16, 20], which does not match the required output shape [10, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [16, 10], which does not match the required output shape [10, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [16, 10], which does not match the required output shape [10, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [16, 20], which does not match the required output shape [10, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [16, 20], which does not match the required output shape [10, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [10, 20], which does not match the required output shape [6, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [10, 10], which does not match the required output shape [6, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [10, 10], which does not match the required output shape [6, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [10, 20], which does not match the required output shape [6, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [10, 20], which does not match the required output shape [6, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [4, 20], which does not match the required output shape [3, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [4, 10], which does not match the required output shape [3, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [4, 10], which does not match the required output shape [3, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [4, 20], which does not match the required output shape [3, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [4, 20], which does not match the required output shape [3, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [0].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [37].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [37].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [35].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [35].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [51].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [51].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [53, 20], which does not match the required output shape [49, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [53, 10], which does not match the required output shape [49, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [53, 10], which does not match the required output shape [49, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [53, 20], which does not match the required output shape [49, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [53, 20], which does not match the required output shape [49, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [51], which does not match the required output shape [59].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [51], which does not match the required output shape [59].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [49, 20], which does not match the required output shape [44, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [49, 10], which does not match the required output shape [44, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [49, 10], which does not match the required output shape [44, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [49, 20], which does not match the required output shape [44, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [49, 20], which does not match the required output shape [44, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [59], which does not match the required output shape [52].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [59], which does not match the required output shape [52].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [44, 20], which does not match the required output shape [40, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [44, 10], which does not match the required output shape [40, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [44, 10], which does not match the required output shape [40, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [44, 20], which does not match the required output shape [40, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [44, 20], which does not match the required output shape [40, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [52], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [52], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [40, 20], which does not match the required output shape [37, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [40, 10], which does not match the required output shape [37, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [40, 10], which does not match the required output shape [37, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [40, 20], which does not match the required output shape [37, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [40, 20], which does not match the required output shape [37, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [37, 20], which does not match the required output shape [35, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [37, 10], which does not match the required output shape [35, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [37, 10], which does not match the required output shape [35, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [37, 20], which does not match the required output shape [35, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [37, 20], which does not match the required output shape [35, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [35, 20], which does not match the required output shape [34, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [35, 10], which does not match the required output shape [34, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [35, 10], which does not match the required output shape [34, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [35, 20], which does not match the required output shape [34, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [35, 20], which does not match the required output shape [34, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [46].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [46].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [34, 20], which does not match the required output shape [28, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [34, 10], which does not match the required output shape [28, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [34, 10], which does not match the required output shape [28, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [34, 20], which does not match the required output shape [28, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [34, 20], which does not match the required output shape [28, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [46], which does not match the required output shape [34].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [46], which does not match the required output shape [34].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [28, 20], which does not match the required output shape [25, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [28, 10], which does not match the required output shape [25, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [28, 10], which does not match the required output shape [25, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [28, 20], which does not match the required output shape [25, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [28, 20], which does not match the required output shape [25, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [35].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [35].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [25, 20], which does not match the required output shape [20, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [25, 10], which does not match the required output shape [20, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [25, 10], which does not match the required output shape [20, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [25, 20], which does not match the required output shape [20, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [25, 20], which does not match the required output shape [20, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [20, 20], which does not match the required output shape [17, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [20, 10], which does not match the required output shape [17, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [20, 10], which does not match the required output shape [17, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [20, 20], which does not match the required output shape [17, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [20, 20], which does not match the required output shape [17, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [17, 20], which does not match the required output shape [11, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [17, 10], which does not match the required output shape [11, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [17, 10], which does not match the required output shape [11, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [17, 20], which does not match the required output shape [11, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [17, 20], which does not match the required output shape [11, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [28].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [28].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [11, 20], which does not match the required output shape [9, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [11, 10], which does not match the required output shape [9, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [11, 10], which does not match the required output shape [9, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [11, 20], which does not match the required output shape [9, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [11, 20], which does not match the required output shape [9, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [1, 20], which does not match the required output shape [48, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [1, 20], which does not match the required output shape [48, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [0].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [45].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [45].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [45], which does not match the required output shape [29].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [45], which does not match the required output shape [29].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [46, 20], which does not match the required output shape [45, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [46, 10], which does not match the required output shape [45, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [46, 10], which does not match the required output shape [45, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [46, 20], which does not match the required output shape [45, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [46, 20], which does not match the required output shape [45, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [36].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [36].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [45, 20], which does not match the required output shape [41, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [45, 10], which does not match the required output shape [41, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [45, 10], which does not match the required output shape [41, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [45, 20], which does not match the required output shape [41, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [45, 20], which does not match the required output shape [41, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [41, 20], which does not match the required output shape [38, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [41, 10], which does not match the required output shape [38, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [41, 10], which does not match the required output shape [38, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [41, 20], which does not match the required output shape [38, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [41, 20], which does not match the required output shape [38, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [38, 20], which does not match the required output shape [37, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [38, 10], which does not match the required output shape [37, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [38, 10], which does not match the required output shape [37, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [38, 20], which does not match the required output shape [37, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [38, 20], which does not match the required output shape [37, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [35, 20], which does not match the required output shape [33, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [35, 10], which does not match the required output shape [33, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [35, 10], which does not match the required output shape [33, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [35, 20], which does not match the required output shape [33, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [35, 20], which does not match the required output shape [33, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [33, 20], which does not match the required output shape [31, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [33, 10], which does not match the required output shape [31, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [33, 10], which does not match the required output shape [31, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [33, 20], which does not match the required output shape [31, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [33, 20], which does not match the required output shape [31, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [31, 20], which does not match the required output shape [29, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [31, 10], which does not match the required output shape [29, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [31, 10], which does not match the required output shape [29, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [31, 20], which does not match the required output shape [29, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [31, 20], which does not match the required output shape [29, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [34].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [34].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [29, 20], which does not match the required output shape [25, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [29, 10], which does not match the required output shape [25, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [29, 10], which does not match the required output shape [25, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [29, 20], which does not match the required output shape [25, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [29, 20], which does not match the required output shape [25, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [25, 20], which does not match the required output shape [22, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [25, 10], which does not match the required output shape [22, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [25, 10], which does not match the required output shape [22, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [25, 20], which does not match the required output shape [22, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [25, 20], which does not match the required output shape [22, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [22, 20], which does not match the required output shape [20, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [22, 10], which does not match the required output shape [20, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [22, 10], which does not match the required output shape [20, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [22, 20], which does not match the required output shape [20, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [22, 20], which does not match the required output shape [20, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [17, 20], which does not match the required output shape [16, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [17, 10], which does not match the required output shape [16, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [17, 10], which does not match the required output shape [16, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [17, 20], which does not match the required output shape [16, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [17, 20], which does not match the required output shape [16, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [25].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [25].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [16, 20], which does not match the required output shape [13, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [16, 10], which does not match the required output shape [13, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [16, 10], which does not match the required output shape [13, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [16, 20], which does not match the required output shape [13, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [16, 20], which does not match the required output shape [13, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [13, 20], which does not match the required output shape [11, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [13, 10], which does not match the required output shape [11, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [13, 10], which does not match the required output shape [11, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [13, 20], which does not match the required output shape [11, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [13, 20], which does not match the required output shape [11, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [9, 20], which does not match the required output shape [5, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [9, 10], which does not match the required output shape [5, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [9, 10], which does not match the required output shape [5, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [9, 20], which does not match the required output shape [5, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [9, 20], which does not match the required output shape [5, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [0].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [1, 20], which does not match the required output shape [32, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [1, 20], which does not match the required output shape [32, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [32, 20], which does not match the required output shape [31, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [32, 10], which does not match the required output shape [31, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [32, 10], which does not match the required output shape [31, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [32, 20], which does not match the required output shape [31, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [32, 20], which does not match the required output shape [31, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [31, 20], which does not match the required output shape [30, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [31, 10], which does not match the required output shape [30, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [31, 10], which does not match the required output shape [30, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [31, 20], which does not match the required output shape [30, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [31, 20], which does not match the required output shape [30, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [30, 20], which does not match the required output shape [29, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [30, 10], which does not match the required output shape [29, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [30, 10], which does not match the required output shape [29, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [30, 20], which does not match the required output shape [29, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [30, 20], which does not match the required output shape [29, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [29, 20], which does not match the required output shape [27, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [29, 10], which does not match the required output shape [27, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [29, 10], which does not match the required output shape [27, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [29, 20], which does not match the required output shape [27, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [29, 20], which does not match the required output shape [27, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [27, 20], which does not match the required output shape [26, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [27, 10], which does not match the required output shape [26, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [27, 10], which does not match the required output shape [26, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [27, 20], which does not match the required output shape [26, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [27, 20], which does not match the required output shape [26, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [26, 20], which does not match the required output shape [25, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [26, 10], which does not match the required output shape [25, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [26, 10], which does not match the required output shape [25, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [26, 20], which does not match the required output shape [25, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [26, 20], which does not match the required output shape [25, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [25].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [25].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [25, 20], which does not match the required output shape [24, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [25, 10], which does not match the required output shape [24, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [25, 10], which does not match the required output shape [24, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [25, 20], which does not match the required output shape [24, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [25, 20], which does not match the required output shape [24, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [24, 20], which does not match the required output shape [22, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [24, 10], which does not match the required output shape [22, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [24, 10], which does not match the required output shape [22, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [24, 20], which does not match the required output shape [22, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [24, 20], which does not match the required output shape [22, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [22, 20], which does not match the required output shape [21, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [22, 10], which does not match the required output shape [21, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [22, 10], which does not match the required output shape [21, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [22, 20], which does not match the required output shape [21, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [22, 20], which does not match the required output shape [21, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [21, 20], which does not match the required output shape [20, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [21, 10], which does not match the required output shape [20, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [21, 10], which does not match the required output shape [20, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [21, 20], which does not match the required output shape [20, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [21, 20], which does not match the required output shape [20, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [18, 20], which does not match the required output shape [15, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [18, 10], which does not match the required output shape [15, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [18, 10], which does not match the required output shape [15, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [18, 20], which does not match the required output shape [15, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [18, 20], which does not match the required output shape [15, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [15, 20], which does not match the required output shape [14, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [15, 10], which does not match the required output shape [14, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [15, 10], which does not match the required output shape [14, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [15, 20], which does not match the required output shape [14, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [15, 20], which does not match the required output shape [14, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [14, 20], which does not match the required output shape [13, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [14, 10], which does not match the required output shape [13, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [14, 10], which does not match the required output shape [13, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [14, 20], which does not match the required output shape [13, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [14, 20], which does not match the required output shape [13, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [13, 20], which does not match the required output shape [9, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [13, 10], which does not match the required output shape [9, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [13, 10], which does not match the required output shape [9, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [13, 20], which does not match the required output shape [9, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [13, 20], which does not match the required output shape [9, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [9, 20], which does not match the required output shape [7, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [9, 10], which does not match the required output shape [7, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [9, 10], which does not match the required output shape [7, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [9, 20], which does not match the required output shape [7, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [9, 20], which does not match the required output shape [7, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [1, 20], which does not match the required output shape [10, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [1, 20], which does not match the required output shape [10, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [10, 20], which does not match the required output shape [9, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [10, 10], which does not match the required output shape [9, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [10, 10], which does not match the required output shape [9, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [10, 20], which does not match the required output shape [9, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [10, 20], which does not match the required output shape [9, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [9, 20], which does not match the required output shape [8, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [9, 10], which does not match the required output shape [8, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
2022-10-14 13:18:08 | INFO | test | epoch 077 | valid on 'test' subset | loss 4.537 | nll_loss 3.132 | ppl 8.765 | bleu 12.88 | wps 1792.8 | wpb 1086.7 | bsz 64.2 | num_updates 8547 | best_bleu 12.88
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [9, 10], which does not match the required output shape [8, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [9, 20], which does not match the required output shape [8, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [9, 20], which does not match the required output shape [8, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:435: UserWarning: An output with one or more elements was resized since it had shape [8, 20], which does not match the required output shape [6, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_mask,
/home/tangli/TSPNet/fairseq/sequence_generator.py:443: UserWarning: An output with one or more elements was resized since it had shape [8, 10], which does not match the required output shape [6, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(new_blacklist, active_hypos)
/home/tangli/TSPNet/fairseq/sequence_generator.py:453: UserWarning: An output with one or more elements was resized since it had shape [8, 10], which does not match the required output shape [6, 10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=active_bbsz_idx,
/home/tangli/TSPNet/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [8, 20], which does not match the required output shape [6, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=(self.scores_buf, self.indices_buf),
/home/tangli/TSPNet/fairseq/search.py:81: UserWarning: An output with one or more elements was resized since it had shape [8, 20], which does not match the required output shape [6, 20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  torch.div(self.indices_buf, vocab_size, rounding_mode="floor",out=self.beams_buf)
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
/home/tangli/TSPNet/fairseq/sequence_generator.py:378: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_bbsz_idx,
/home/tangli/TSPNet/fairseq/sequence_generator.py:386: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)
  out=eos_scores,
precisions: ['35.52', '14.59', '8.65', '6.14']
test set has 642 samples,
sacrebleu: BLEU = 12.88 35.5/14.6/8.7/6.1 (BP = 1.000 ratio = 1.005 hyp_len = 8474 ref_len = 8428),
CVPR BLEU scripts: [35.52, 22.77, 16.49, 12.88]
CVPR ROUGE: {'rouge_1/f_score': 0.38565370756792516, 'rouge_1/r_score': 0.38042865229540146, 'rouge_1/p_score': 0.4039893632940253, 'rouge_2/f_score': 0.16722138075681314, 'rouge_2/r_score': 0.1650624030470623, 'rouge_2/p_score': 0.17347459845348542, 'rouge_l/f_score': 0.341214556202006, 'rouge_l/r_score': 0.3531404524395082, 'rouge_l/p_score': 0.3601846300954075}
performance: 34.12 35.52 22.77 16.49 12.88
